import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.utils import resample
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
import warnings
warnings.filterwarnings('ignore')

print("=== OPTIMIZED XGBOOST FOR 4 KEY FEATURES BLOOD CANCER CLASSIFICATION ===")

# Load the clean processed dataset
try:
    df = pd.read_csv('public/processed_data/clean_4_features_dataset.csv')
    print(f"âœ… Clean dataset loaded successfully!")
    print(f"Dataset shape: {df.shape}")
    print(f"Columns: {df.columns.tolist()}")
except FileNotFoundError:
    print("âŒ Error: Could not find the clean processed dataset file.")
    print("Please run data_preprocessing.py first.")
    exit()

print(f"\n=== INITIAL DATA ANALYSIS ===")
print(f"Dataset shape: {df.shape}")
print(f"\nTarget variable distribution:")
print(df['Cancer_Type(AML, ALL, CLL)'].value_counts())
print(f"\nData types:")
print(df.dtypes)
print(f"\nBasic statistics:")
print(df.describe())

# STEP 1: ENHANCED FEATURE ENGINEERING
print(f"\n=== ENHANCED FEATURE ENGINEERING ===")

# Create working copy
df_processed = df.copy()

# 1. Basic ratios and transformations
df_processed['WBC_Platelet_Ratio'] = df_processed['Total WBC count(/cumm)'] / (df_processed['Platelet Count( (/cumm)'] + 1)
df_processed['Platelet_WBC_Ratio'] = df_processed['Platelet Count( (/cumm)'] / (df_processed['Total WBC count(/cumm)'] + 1)

# 2. Age-based features
df_processed['Age_Group'] = pd.cut(df_processed['Age'], bins=[0, 20, 40, 60, 100], labels=[0, 1, 2, 3])
df_processed['Age_Group'] = df_processed['Age_Group'].astype(int)
df_processed['Age_Squared'] = df_processed['Age'] ** 2
df_processed['Age_Sqrt'] = np.sqrt(df_processed['Age'])

# 3. Statistical thresholds based on cancer type knowledge
wbc_median = df_processed['Total WBC count(/cumm)'].median()
platelet_median = df_processed['Platelet Count( (/cumm)'].median()
age_median = df_processed['Age'].median()

df_processed['High_WBC'] = (df_processed['Total WBC count(/cumm)'] > wbc_median).astype(int)
df_processed['Low_WBC'] = (df_processed['Total WBC count(/cumm)'] < wbc_median * 0.5).astype(int)
df_processed['High_Platelet'] = (df_processed['Platelet Count( (/cumm)'] > platelet_median).astype(int)
df_processed['Low_Platelet'] = (df_processed['Platelet Count( (/cumm)'] < platelet_median * 0.5).astype(int)

# 4. Interaction features
df_processed['WBC_Age_Interaction'] = df_processed['Total WBC count(/cumm)'] * df_processed['Age']
df_processed['Platelet_Age_Interaction'] = df_processed['Platelet Count( (/cumm)'] * df_processed['Age']
df_processed['WBC_Platelet_Product'] = df_processed['Total WBC count(/cumm)'] * df_processed['Platelet Count( (/cumm)']

# 5. Log transformations for skewed distributions
df_processed['Log_WBC'] = np.log1p(df_processed['Total WBC count(/cumm)'])
df_processed['Log_Platelet'] = np.log1p(df_processed['Platelet Count( (/cumm)'])
df_processed['Log_Age'] = np.log1p(df_processed['Age'])

# 6. Normalization features (z-scores)
df_processed['WBC_Zscore'] = (df_processed['Total WBC count(/cumm)'] - df_processed['Total WBC count(/cumm)'].mean()) / df_processed['Total WBC count(/cumm)'].std()
df_processed['Platelet_Zscore'] = (df_processed['Platelet Count( (/cumm)'] - df_processed['Platelet Count( (/cumm)'].mean()) / df_processed['Platelet Count( (/cumm)'].std()
df_processed['Age_Zscore'] = (df_processed['Age'] - df_processed['Age'].mean()) / df_processed['Age'].std()

# 7. Combined risk scores based on medical knowledge
df_processed['Blood_Count_Risk_Score'] = (
    df_processed['High_WBC'] * 2 + 
    df_processed['Low_Platelet'] * 2 + 
    df_processed['Low_WBC'] * 1 + 
    df_processed['High_Platelet'] * 0.5
)

# 8. Power transformations
df_processed['WBC_Power_2'] = df_processed['Total WBC count(/cumm)'] ** 2
df_processed['Platelet_Power_2'] = df_processed['Platelet Count( (/cumm)'] ** 2
df_processed['WBC_Sqrt'] = np.sqrt(df_processed['Total WBC count(/cumm)'])
df_processed['Platelet_Sqrt'] = np.sqrt(df_processed['Platelet Count( (/cumm)'])

# 9. Binning features for non-linear patterns
df_processed['WBC_Bins'] = pd.qcut(df_processed['Total WBC count(/cumm)'], q=5, labels=[0, 1, 2, 3, 4]).astype(int)
df_processed['Platelet_Bins'] = pd.qcut(df_processed['Platelet Count( (/cumm)'], q=5, labels=[0, 1, 2, 3, 4]).astype(int)
df_processed['Age_Bins'] = pd.qcut(df_processed['Age'], q=5, labels=[0, 1, 2, 3, 4]).astype(int)

print(f"Total features after engineering: {len(df_processed.columns)}")
print(f"New features created: {len(df_processed.columns) - 4}")

# STEP 2: PREPARE FEATURES AND TARGET
X = df_processed.drop('Cancer_Type(AML, ALL, CLL)', axis=1)
y = df_processed['Cancer_Type(AML, ALL, CLL)']

# Encode target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
target_names = label_encoder.classes_

print(f"Feature matrix shape: {X.shape}")
print(f"Target classes: {target_names}")
print(f"Class distribution: {pd.Series(y_encoded).value_counts().sort_index()}")

# STEP 3: INTELLIGENT CLASS BALANCING
print(f"\n=== INTELLIGENT CLASS BALANCING ===")

def enhanced_balance_classes(X, y):
    """Enhanced class balancing with controlled oversampling"""
    df_combined = pd.concat([X.reset_index(drop=True), pd.Series(y, name='target').reset_index(drop=True)], axis=1)
    
    class_counts = df_combined['target'].value_counts()
    print(f"Original class distribution: {class_counts.to_dict()}")
    
    # Use the size of the largest class as target
    target_size = class_counts.max()
    
    balanced_dfs = []
    for class_val in sorted(df_combined['target'].unique()):
        class_df = df_combined[df_combined['target'] == class_val]
        
        if len(class_df) < target_size:
            # Oversample with slight noise injection
            oversampled = resample(class_df, replace=True, n_samples=target_size, random_state=42)
            # Add small amount of noise to numeric features to create diversity
            numeric_cols = X.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if col in oversampled.columns:
                    noise_factor = oversampled[col].std() * 0.005  # Very small noise
                    noise = np.random.normal(0, noise_factor, len(oversampled))
                    oversampled[col] += noise
            balanced_dfs.append(oversampled)
        else:
            balanced_dfs.append(class_df)
    
    df_balanced = pd.concat(balanced_dfs, ignore_index=True)
    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)
    
    X_balanced = df_balanced.drop('target', axis=1)
    y_balanced = df_balanced['target'].values
    
    balanced_counts = pd.Series(y_balanced).value_counts()
    print(f"Balanced class distribution: {balanced_counts.to_dict()}")
    
    return X_balanced, y_balanced

X_balanced, y_balanced = enhanced_balance_classes(X, y_encoded)

# STEP 4: ADVANCED FEATURE SELECTION
print(f"\n=== ADVANCED FEATURE SELECTION ===")

# Method 1: Mutual Information
selector_mi = SelectKBest(score_func=mutual_info_classif, k=min(20, X_balanced.shape[1]))
X_selected_mi = selector_mi.fit_transform(X_balanced, y_balanced)
selected_features_mi = X_balanced.columns[selector_mi.get_support()]

# Method 2: F-statistics
selector_f = SelectKBest(score_func=f_classif, k=min(20, X_balanced.shape[1]))
X_selected_f = selector_f.fit_transform(X_balanced, y_balanced)
selected_features_f = X_balanced.columns[selector_f.get_support()]

# Method 3: XGBoost feature importance
temp_xgb = XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')
temp_xgb.fit(X_balanced, y_balanced)
importance_scores = temp_xgb.feature_importances_
top_features_xgb = X_balanced.columns[np.argsort(importance_scores)[-20:][::-1]]

# Combine feature selection methods - take intersection of top features
combined_features = list(set(selected_features_mi) & set(selected_features_f) & set(top_features_xgb))

# If intersection is too small, take union of top features
if len(combined_features) < 15:
    combined_features = list(set(selected_features_mi) | set(selected_features_f) | set(top_features_xgb))
    # Limit to top 25 features to avoid overfitting
    if len(combined_features) > 25:
        # Get top features based on XGBoost importance
        feature_importance_dict = dict(zip(X_balanced.columns, importance_scores))
        combined_features = sorted(combined_features, key=lambda x: feature_importance_dict[x], reverse=True)[:25]

print(f"Features selected by Mutual Information: {len(selected_features_mi)}")
print(f"Features selected by F-statistics: {len(selected_features_f)}")
print(f"Features selected by XGBoost importance: {len(top_features_xgb)}")
print(f"Final selected features: {len(combined_features)}")
print(f"Selected features: {combined_features}")

# Use selected features
X_selected = X_balanced[combined_features]

# STEP 5: ROBUST SCALING
print(f"\n=== ROBUST SCALING ===")
scaler = RobustScaler()  # More robust to outliers than StandardScaler
X_scaled = scaler.fit_transform(X_selected)

# STEP 6: STRATIFIED TRAIN-TEST SPLIT
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced
)

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

# === OPTIMIZED XGBOOST TRAINING ===
print(f"\n=== OPTIMIZED XGBOOST TRAINING ===")

# Stage 1: Initial parameter exploration
print("Stage 1: Initial parameter exploration...")
initial_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 6, 8],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

cv_initial = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

xgb_model_initial = XGBClassifier(
    eval_metric='mlogloss',
    random_state=42,
    tree_method='hist',
    objective='multi:softprob',
    n_jobs=-1
)

initial_grid_search = GridSearchCV(
    xgb_model_initial,
    initial_param_grid,
    scoring='accuracy',
    cv=cv_initial,
    n_jobs=-1,
    verbose=1
)

initial_grid_search.fit(X_train, y_train)
print(f"Initial search best params: {initial_grid_search.best_params_}")
print(f"Initial search best score: {initial_grid_search.best_score_:.4f}")

# Stage 2: Fine-tuning around best parameters
print("\nStage 2: Fine-tuning best parameters...")
best_params = initial_grid_search.best_params_

# Create a refined grid around the best parameters
refined_param_grid = {
    'n_estimators': [best_params['n_estimators'], min(500, best_params['n_estimators'] + 100)],
    'max_depth': [max(3, best_params['max_depth'] - 1), best_params['max_depth'], min(10, best_params['max_depth'] + 1)],
    'learning_rate': [best_params['learning_rate'] * 0.8, best_params['learning_rate'], best_params['learning_rate'] * 1.2],
    'subsample': [best_params['subsample']],
    'colsample_bytree': [best_params['colsample_bytree']],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.1, 0.5],
    'reg_lambda': [1, 1.5, 2],
    'min_child_weight': [1, 3, 5]
}

# Limit learning rate values to valid range
refined_param_grid['learning_rate'] = [max(0.01, min(0.3, lr)) for lr in refined_param_grid['learning_rate']]

cv_final = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

xgb_model_final = XGBClassifier(
    eval_metric='mlogloss',
    random_state=42,
    tree_method='hist',
    objective='multi:softprob',
    n_jobs=-1
)

final_grid_search = GridSearchCV(
    xgb_model_final,
    refined_param_grid,
    scoring='accuracy',
    cv=cv_final,
    n_jobs=-1,
    verbose=1
)

final_grid_search.fit(X_train, y_train)
best_xgb = final_grid_search.best_estimator_

print(f"Final best params: {final_grid_search.best_params_}")
print(f"Final best CV score: {final_grid_search.best_score_:.4f}")

# Stage 3: Ensemble approach
print("\nStage 3: Creating ensemble model...")

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=8,
    random_state=42,
    n_jobs=-1
)

ensemble_model = VotingClassifier(
    estimators=[
        ('xgb', best_xgb),
        ('rf', rf_model)
    ],
    voting='soft',
    n_jobs=-1
)

print("Training ensemble model...")
ensemble_model.fit(X_train, y_train)

# === MODEL EVALUATION ===
print(f"\n=== MODEL EVALUATION ===")

# Compare models
xgb_pred = best_xgb.predict(X_test)
ensemble_pred = ensemble_model.predict(X_test)

xgb_accuracy = accuracy_score(y_test, xgb_pred)
ensemble_accuracy = accuracy_score(y_test, ensemble_pred)

xgb_f1 = f1_score(y_test, xgb_pred, average='weighted')
ensemble_f1 = f1_score(y_test, ensemble_pred, average='weighted')

print(f"XGBoost - Accuracy: {xgb_accuracy:.4f}, F1: {xgb_f1:.4f}")
print(f"Ensemble - Accuracy: {ensemble_accuracy:.4f}, F1: {ensemble_f1:.4f}")

# Select best model
if ensemble_accuracy > xgb_accuracy:
    print("ðŸ† Using ensemble model (better performance)")
    best_model = ensemble_model
    y_pred_final = ensemble_pred
    final_accuracy = ensemble_accuracy
    final_f1 = ensemble_f1
    model_type = "Ensemble"
else:
    print("ðŸ† Using XGBoost model (better performance)")
    best_model = best_xgb
    y_pred_final = xgb_pred
    final_accuracy = xgb_accuracy
    final_f1 = xgb_f1
    model_type = "XGBoost"

# Training accuracy check
y_train_pred = best_model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)

print(f"\nðŸŽ¯ FINAL RESULTS:")
print(f"Model Type: {model_type}")
print(f"Test Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)")
print(f"Test F1-Score: {final_f1:.4f}")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Overfitting Check: {train_accuracy - final_accuracy:.4f}")

# Cross-validation
cv_scores = cross_val_score(best_model, X_scaled, y_balanced, cv=5, scoring='accuracy')
print(f"CV Mean Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# Detailed classification report
print(f"\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred_final, target_names=target_names))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_final)
print(f"\nðŸ“Š Confusion Matrix:")
print(cm)

# Feature importance
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'Feature': combined_features,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
elif hasattr(best_model, 'named_estimators_'):
    # For ensemble, get XGBoost importance
    xgb_estimator = best_model.named_estimators_['xgb']
    feature_importance = pd.DataFrame({
        'Feature': combined_features,
        'Importance': xgb_estimator.feature_importances_
    }).sort_values('Importance', ascending=False)

print(f"\nðŸ“Š Top 15 Feature Importances:")
print(feature_importance.head(15))

# === SAVE MODELS AND RESULTS ===
print(f"\nðŸ’¾ SAVING MODELS AND RESULTS...")

# Create output directory
os.makedirs('public/optimized_4_features_analysis', exist_ok=True)

# Save models
if model_type == "Ensemble":
    joblib.dump(best_model, 'public/optimized_4_features_analysis/ensemble_model.pkl')
    joblib.dump(best_xgb, 'public/optimized_4_features_analysis/xgboost_model.pkl')
else:
    joblib.dump(best_model, 'public/optimized_4_features_analysis/xgboost_model.pkl')

# Save preprocessing components
joblib.dump(scaler, 'public/optimized_4_features_analysis/scaler.pkl')
joblib.dump(label_encoder, 'public/optimized_4_features_analysis/label_encoder.pkl')

# Save feature information
feature_info = {
    'selected_features': combined_features,
    'original_features': ["Age", "Platelet Count( (/cumm)", "Total WBC count(/cumm)"],
    'engineered_features': list(set(combined_features) - set(["Age", "Platelet Count( (/cumm)", "Total WBC count(/cumm)"]))
}
joblib.dump(feature_info, 'public/optimized_4_features_analysis/feature_info.pkl')

# Save feature importance
feature_importance.to_csv('public/optimized_4_features_analysis/feature_importance.csv', index=False)

# === VISUALIZATIONS ===
print(f"\nðŸ“Š CREATING VISUALIZATIONS...")

# 1. Feature Importance Plot
plt.figure(figsize=(12, 8))
top_features = feature_importance.head(15)
sns.barplot(data=top_features, x='Importance', y='Feature', palette='viridis')
plt.title(f'Top 15 Feature Importances - {model_type} Model\\nAccuracy: {final_accuracy*100:.2f}%', fontsize=16)
plt.xlabel('Importance Score', fontsize=12)
plt.tight_layout()
plt.savefig('public/optimized_4_features_analysis/feature_importance_plot.png', dpi=300, bbox_inches='tight')
plt.show()

# 2. Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.title(f'Confusion Matrix - {model_type} Model\\nAccuracy: {final_accuracy*100:.2f}%', fontsize=16)
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('Actual', fontsize=12)
plt.tight_layout()
plt.savefig('public/optimized_4_features_analysis/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# 3. Performance Metrics
plt.figure(figsize=(12, 6))
metrics_data = {
    'Metric': ['Training Accuracy', 'Test Accuracy', 'CV Mean Accuracy', 'F1-Score'],
    'Score': [train_accuracy, final_accuracy, cv_scores.mean(), final_f1]
}
metrics_df = pd.DataFrame(metrics_data)

sns.barplot(data=metrics_df, x='Metric', y='Score', palette='Set2')
plt.title(f'Model Performance Metrics - {model_type}', fontsize=16)
plt.ylabel('Score', fontsize=12)
plt.ylim(0, 1)
for i, v in enumerate(metrics_df['Score']):
    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=10)
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('public/optimized_4_features_analysis/performance_metrics.png', dpi=300, bbox_inches='tight')
plt.show()

# 4. Cross-validation scores
plt.figure(figsize=(10, 6))
plt.bar(range(len(cv_scores)), cv_scores, alpha=0.7)
plt.axhline(y=cv_scores.mean(), color='red', linestyle='--', label=f'Mean: {cv_scores.mean():.4f}')
plt.title('Cross-Validation Accuracy Scores', fontsize=16)
plt.xlabel('Fold', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.legend()
plt.tight_layout()
plt.savefig('public/optimized_4_features_analysis/cv_scores.png', dpi=300, bbox_inches='tight')
plt.show()

# === SUMMARY REPORT ===
summary_report = {
    'Dataset_Info': {
        'Original_Shape': df.shape,
        'Final_Shape': df_processed.shape,
        'Features_Engineered': len(df_processed.columns) - 4,
        'Selected_Features': len(combined_features)
    },
    'Model_Performance': {
        'Model_Type': model_type,
        'Test_Accuracy': float(final_accuracy),
        'Test_F1_Score': float(final_f1),
        'Training_Accuracy': float(train_accuracy),
        'CV_Mean_Accuracy': float(cv_scores.mean()),
        'CV_Std_Accuracy': float(cv_scores.std()),
        'Overfitting_Check': float(train_accuracy - final_accuracy),
        'XGBoost_Accuracy': float(xgb_accuracy),
        'Ensemble_Accuracy': float(ensemble_accuracy) if 'ensemble_accuracy' in locals() else None
    },
    'Best_Parameters': final_grid_search.best_params_,
    'Feature_Engineering': {
        'Original_Features': ["Age", "Platelet Count( (/cumm)", "Total WBC count(/cumm)"],
        'Engineered_Features': list(set(combined_features) - set(["Age", "Platelet Count( (/cumm)", "Total WBC count(/cumm)"]))[:10],  # Top 10
        'Feature_Selection_Methods': ['Mutual Information', 'F-Statistics', 'XGBoost Importance']
    },
    'Top_15_Features': feature_importance.head(15)[['Feature', 'Importance']].to_dict('records'),
    'Cancer_Types': target_names.tolist(),
    'Optimization_Techniques': [
        'Two-stage hyperparameter tuning',
        'Ensemble voting classifier',
        'Advanced feature engineering',
        'Intelligent class balancing',
        'Robust scaling',
        'Multiple feature selection methods',
        'Medical knowledge-based feature creation',
        'Noise injection for diversity in oversampling'
    ]
}

# Save summary
with open('public/optimized_4_features_analysis/analysis_summary.json', 'w') as f:
    json.dump(summary_report, f, indent=2)

# === PREDICTION FUNCTION ===
def predict_cancer_type(age, platelet_count, wbc_count):
    """
    Predict cancer type given the 3 main features.
    
    Args:
        age: Patient age
        platelet_count: Platelet count per cumm
        wbc_count: WBC count per cumm
    
    Returns:
        Predicted cancer type
    """
    # Create a dataframe with the input
    input_data = pd.DataFrame({
        'Age': [age],
        'Platelet Count( (/cumm)': [platelet_count],
        'Total WBC count(/cumm)': [wbc_count]
    })
    
    # Apply the same feature engineering
    input_data['WBC_Platelet_Ratio'] = input_data['Total WBC count(/cumm)'] / (input_data['Platelet Count( (/cumm)'] + 1)
    input_data['Platelet_WBC_Ratio'] = input_data['Platelet Count( (/cumm)'] / (input_data['Total WBC count(/cumm)'] + 1)
    input_data['Age_Group'] = pd.cut(input_data['Age'], bins=[0, 20, 40, 60, 100], labels=[0, 1, 2, 3]).astype(int)
    input_data['Age_Squared'] = input_data['Age'] ** 2
    input_data['Age_Sqrt'] = np.sqrt(input_data['Age'])
    
    # Add other engineered features (using training data statistics)
    wbc_median = 53754.5  # From training data
    platelet_median = 208439.5  # From training data
    
    input_data['High_WBC'] = (input_data['Total WBC count(/cumm)'] > wbc_median).astype(int)
    input_data['Low_WBC'] = (input_data['Total WBC count(/cumm)'] < wbc_median * 0.5).astype(int)
    input_data['High_Platelet'] = (input_data['Platelet Count( (/cumm)'] > platelet_median).astype(int)
    input_data['Low_Platelet'] = (input_data['Platelet Count( (/cumm)'] < platelet_median * 0.5).astype(int)
    
    input_data['WBC_Age_Interaction'] = input_data['Total WBC count(/cumm)'] * input_data['Age']
    input_data['Platelet_Age_Interaction'] = input_data['Platelet Count( (/cumm)'] * input_data['Age']
    input_data['WBC_Platelet_Product'] = input_data['Total WBC count(/cumm)'] * input_data['Platelet Count( (/cumm)']
    
    input_data['Log_WBC'] = np.log1p(input_data['Total WBC count(/cumm)'])
    input_data['Log_Platelet'] = np.log1p(input_data['Platelet Count( (/cumm)'])
    input_data['Log_Age'] = np.log1p(input_data['Age'])
    
    # Add z-scores using training statistics
    wbc_mean, wbc_std = 52011.51, 28712.62  # From training data
    platelet_mean, platelet_std = 209532.15, 109187.48  # From training data
    age_mean, age_std = 45.37, 25.55  # From training data
    
    input_data['WBC_Zscore'] = (input_data['Total WBC count(/cumm)'] - wbc_mean) / wbc_std
    input_data['Platelet_Zscore'] = (input_data['Platelet Count( (/cumm)'] - platelet_mean) / platelet_std
    input_data['Age_Zscore'] = (input_data['Age'] - age_mean) / age_std
    
    input_data['Blood_Count_Risk_Score'] = (
        input_data['High_WBC'] * 2 + 
        input_data['Low_Platelet'] * 2 + 
        input_data['Low_WBC'] * 1 + 
        input_data['High_Platelet'] * 0.5
    )
    
    input_data['WBC_Power_2'] = input_data['Total WBC count(/cumm)'] ** 2
    input_data['Platelet_Power_2'] = input_data['Platelet Count( (/cumm)'] ** 2
    input_data['WBC_Sqrt'] = np.sqrt(input_data['Total WBC count(/cumm)'])
    input_data['Platelet_Sqrt'] = np.sqrt(input_data['Platelet Count( (/cumm)'])
    
    # Add binning features
    input_data['WBC_Bins'] = 2  # Default to middle bin
    input_data['Platelet_Bins'] = 2  # Default to middle bin
    input_data['Age_Bins'] = 2  # Default to middle bin
    
    # Select only the features used in training
    input_features = input_data[combined_features]
    
    # Scale features
    input_scaled = scaler.transform(input_features)
    
    # Predict using the best model
    prediction = best_model.predict(input_scaled)
    prediction_proba = best_model.predict_proba(input_scaled)
    
    predicted_class = label_encoder.inverse_transform(prediction)[0]
    confidence = max(prediction_proba[0])
    
    return predicted_class, confidence

# Example usage
print(f"\nðŸ”® EXAMPLE PREDICTION:")
example_age = 45
example_platelet = 200000
example_wbc = 55000

predicted_cancer_type, confidence = predict_cancer_type(example_age, example_platelet, example_wbc)
print(f"Age: {example_age}, Platelet Count: {example_platelet}, WBC Count: {example_wbc}")
print(f"Predicted Cancer Type: {predicted_cancer_type}")
print(f"Confidence: {confidence:.3f}")

print("ðŸ’¾ All results saved successfully in 'public/optimized_4_features_analysis/' directory.")

# Save final metrics
metrics = {
    'accuracy': final_accuracy,
    'f1_score': final_f1,
    'train_accuracy': train_accuracy,
    'cv_mean_accuracy': cv_scores.mean(),
    'cv_std_accuracy': cv_scores.std(),
    'best_params': final_grid_search.best_params_,
    'model_type': model_type,
    'feature_importance': feature_importance.to_dict(orient='records'),
    'selected_features': combined_features,
    'cancer_types': target_names.tolist(),
    'summary_report': summary_report
}

metrics_file = 'public/optimized_4_features_analysis/model_metrics.json'
with open(metrics_file, 'w') as f:
    json.dump(metrics, f, indent=2)

print(f"âœ… Model metrics saved to: {metrics_file}")
print("ðŸŽ‰ Analysis complete! Optimized XGBoost model for 4-feature blood cancer classification is ready.")
print("ðŸ† Key Results:")
print(f"   â€¢ Test Accuracy: {final_accuracy*100:.2f}%")
print(f"   â€¢ F1-Score: {final_f1:.3f}")
print(f"   â€¢ Model Type: {model_type}")
print(f"   â€¢ Features Used: {len(combined_features)} (from 4 original)")
print("Thank you for using the Optimized 4-Feature XGBoost Analysis script!")
